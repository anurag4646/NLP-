{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/kaggle/input/yelp-dataset/Dataset_User_Agreement.pdf\n",
      "/kaggle/input/yelp-dataset/yelp_academic_dataset_review.json\n",
      "/kaggle/input/yelp-dataset/yelp_academic_dataset_checkin.json\n",
      "/kaggle/input/yelp-dataset/yelp_academic_dataset_business.json\n",
      "/kaggle/input/yelp-dataset/yelp_academic_dataset_tip.json\n",
      "/kaggle/input/yelp-dataset/yelp_academic_dataset_user.json\n"
     ]
    }
   ],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n",
    "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
    "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Library "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "REVIEW_PATH = '/kaggle/input/yelp-dataset/yelp_academic_dataset_review.json'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_rows(file_path, nrows=None, only_return_count=False, verbose=True):\n",
    "    \"\"\"\n",
    "    Returns dataframe from json file\n",
    "    \"\"\"\n",
    "    tic = time()\n",
    "    with open(file_path) as json_file:\n",
    "        count = 0\n",
    "        objs = []\n",
    "        line = json_file.readline()\n",
    "        while (nrows is None or count<nrows) and line:\n",
    "            count += 1\n",
    "            if not only_return_count:\n",
    "                obj = json.loads(line)\n",
    "                objs.append(obj)\n",
    "            line = json_file.readline()\n",
    "        toc = time()\n",
    "        if verbose:\n",
    "            print(file_path.split('/')[-1], 'loaded. Count =', count, ', Time =', round(toc-tic,2), 'secs.')     \n",
    "        if only_return_count:\n",
    "            return count       \n",
    "        return pd.DataFrame(objs)    \n",
    "#data generator to load data in chunks\n",
    "def load_rows_gen(file_path, nrows=1e6, verbose=True):\n",
    "    \"\"\"\n",
    "    Returns data in chunks\n",
    "    \"\"\"\n",
    "    with open(file_path) as json_file:\n",
    "        line = json_file.readline()\n",
    "        total = 0\n",
    "        while line:\n",
    "            count = 0\n",
    "            objs = []\n",
    "            tic = time()\n",
    "            while count<nrows and line:\n",
    "                count+=1\n",
    "                obj = json.loads(line)\n",
    "                objs.append(obj)\n",
    "                line = json_file.readline()\n",
    "                total += count\n",
    "            toc = time()\n",
    "            print('Loaded chunk of size:', count, \", Time =\", round(toc-tic,2), 'secs.')\n",
    "            yield pd.DataFrame(objs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "yelp_academic_dataset_review.json loaded. Count = 5 , Time = 0.0 secs.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review_id</th>\n",
       "      <th>user_id</th>\n",
       "      <th>business_id</th>\n",
       "      <th>stars</th>\n",
       "      <th>useful</th>\n",
       "      <th>funny</th>\n",
       "      <th>cool</th>\n",
       "      <th>text</th>\n",
       "      <th>date</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>lWC-xP3rd6obsecCYsGZRg</td>\n",
       "      <td>ak0TdVmGKo4pwqdJSTLwWw</td>\n",
       "      <td>buF9druCkbuXLX526sGELQ</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Apparently Prides Osteria had a rough summer a...</td>\n",
       "      <td>2014-10-11 03:34:02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>8bFej1QE5LXp4O05qjGqXA</td>\n",
       "      <td>YoVfDbnISlW0f7abNQACIg</td>\n",
       "      <td>RA4V8pr014UyUbDvI-LW2A</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>This store is pretty good. Not as great as Wal...</td>\n",
       "      <td>2015-07-03 20:38:25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>NDhkzczKjLshODbqDoNLSg</td>\n",
       "      <td>eC5evKn1TWDyHCyQAwguUw</td>\n",
       "      <td>_sS2LBIGNT5NQb6PD1Vtjw</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>I called WVM on the recommendation of a couple...</td>\n",
       "      <td>2013-05-28 20:38:06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>T5fAqjjFooT4V0OeZyuk1w</td>\n",
       "      <td>SFQ1jcnGguO0LYWnbbftAA</td>\n",
       "      <td>0AzLzHfOJgL7ROwhdww2ew</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>I've stayed at many Marriott and Renaissance M...</td>\n",
       "      <td>2010-01-08 02:29:15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>sjm_uUcQVxab_EeLCqsYLg</td>\n",
       "      <td>0kA0PAJ8QFMeveQWHFqz2A</td>\n",
       "      <td>8zehGz9jnxPqXtOc7KaJxA</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>The food is always great here. The service fro...</td>\n",
       "      <td>2011-07-28 18:05:01</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                review_id                 user_id             business_id  \\\n",
       "0  lWC-xP3rd6obsecCYsGZRg  ak0TdVmGKo4pwqdJSTLwWw  buF9druCkbuXLX526sGELQ   \n",
       "1  8bFej1QE5LXp4O05qjGqXA  YoVfDbnISlW0f7abNQACIg  RA4V8pr014UyUbDvI-LW2A   \n",
       "2  NDhkzczKjLshODbqDoNLSg  eC5evKn1TWDyHCyQAwguUw  _sS2LBIGNT5NQb6PD1Vtjw   \n",
       "3  T5fAqjjFooT4V0OeZyuk1w  SFQ1jcnGguO0LYWnbbftAA  0AzLzHfOJgL7ROwhdww2ew   \n",
       "4  sjm_uUcQVxab_EeLCqsYLg  0kA0PAJ8QFMeveQWHFqz2A  8zehGz9jnxPqXtOc7KaJxA   \n",
       "\n",
       "   stars  useful  funny  cool  \\\n",
       "0    4.0       3      1     1   \n",
       "1    4.0       1      0     0   \n",
       "2    5.0       0      0     0   \n",
       "3    2.0       1      1     1   \n",
       "4    4.0       0      0     0   \n",
       "\n",
       "                                                text                 date  \n",
       "0  Apparently Prides Osteria had a rough summer a...  2014-10-11 03:34:02  \n",
       "1  This store is pretty good. Not as great as Wal...  2015-07-03 20:38:25  \n",
       "2  I called WVM on the recommendation of a couple...  2013-05-28 20:38:06  \n",
       "3  I've stayed at many Marriott and Renaissance M...  2010-01-08 02:29:15  \n",
       "4  The food is always great here. The service fro...  2011-07-28 18:05:01  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = load_rows(REVIEW_PATH, 5)\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import pickle\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /usr/share/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk import FreqDist\n",
    "nltk.download('stopwords') # run this one time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['text'] = data['text'].str.replace(\"[^a-zA-Z#]\", \" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create sequences of length 5 tokens\n",
    "def create_seq(text, seq_len = 5):\n",
    "    \n",
    "    sequences = []\n",
    "\n",
    "    # if the number of tokens in 'text' is greater than 5\n",
    "    if len(text.split()) > seq_len:\n",
    "        \n",
    "        for i in range(seq_len, len(text.split())):\n",
    "        # select sequence of tokens\n",
    "            seq = text.split()[i-seq_len:i+1]\n",
    "        # add to the list\n",
    "            sequences.append(\" \".join(seq))\n",
    "\n",
    "        return sequences\n",
    "\n",
    "    # if the number of tokens in 'text' is less than or equal to 5\n",
    "    else:\n",
    "      \n",
    "      return [text]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "644"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seqs = [create_seq(i) for i in data['text']]\n",
    "\n",
    "# merge list-of-lists into a single list\n",
    "seqs = sum(seqs, [])\n",
    "\n",
    "# count of sequences\n",
    "len(seqs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create inputs and targets (x and y)\n",
    "x = []\n",
    "y = []\n",
    "\n",
    "for s in seqs:\n",
    "  x.append(\" \".join(s.split()[:-1]))\n",
    "  y.append(\" \".join(s.split()[1:]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create integer-to-token mapping\n",
    "int2token = {}\n",
    "cnt = 0\n",
    "\n",
    "for w in set(\" \".join(data[\"text\"]).split()):\n",
    "    \n",
    "    int2token[cnt] = w\n",
    "    cnt+= 1\n",
    "\n",
    "# create token-to-integer mapping\n",
    "token2int = {t: i for i, t in int2token.items()}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(118, 'while')"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token2int[\"process\"], int2token[11]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "371"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# set vocabulary size\n",
    "vocab_size = len(int2token)\n",
    "vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_integer_seq(seq):\n",
    "    \n",
    "    return [token2int[w] for w in seq.split()]\n",
    "\n",
    "# convert text sequences to integer sequences\n",
    "x_int = [get_integer_seq(i) for i in x]\n",
    "y_int = [get_integer_seq(i) for i in y]\n",
    "\n",
    "# convert lists to numpy arrays\n",
    "x_int = np.array(x_int)\n",
    "y_int = np.array(y_int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[128,  54, 131, 226,  10],\n",
       "       [ 54, 131, 226,  10, 250],\n",
       "       [131, 226,  10, 250, 309],\n",
       "       ...,\n",
       "       [164,  81,   1,  92,  58],\n",
       "       [ 81,   1,  92,  58,  10],\n",
       "       [  1,  92,  58,  10,  93]])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_int"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 54, 131, 226,  10, 250],\n",
       "       [131, 226,  10, 250, 309],\n",
       "       [226,  10, 250, 309, 192],\n",
       "       ...,\n",
       "       [ 81,   1,  92,  58,  10],\n",
       "       [  1,  92,  58,  10,  93],\n",
       "       [ 92,  58,  10,  93,  77]])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_int"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Building "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_batches(arr_x, arr_y, batch_size):\n",
    "         \n",
    "    # iterate through the arrays\n",
    "    prv = 0\n",
    "    for n in range(batch_size, arr_x.shape[0], batch_size):\n",
    "        \n",
    "        x = arr_x[prv:n,:]\n",
    "        y = arr_y[prv:n,:]\n",
    "        prv = n\n",
    "        yield x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WordLSTM(nn.Module):\n",
    "    \n",
    "    def __init__(self, n_hidden=256, n_layers=4, drop_prob=0.3, lr=0.001):\n",
    "        super().__init__()\n",
    "\n",
    "        self.drop_prob = drop_prob\n",
    "        self.n_layers = n_layers\n",
    "        self.n_hidden = n_hidden\n",
    "        self.lr = lr\n",
    "        \n",
    "        self.emb_layer = nn.Embedding(vocab_size, 200)\n",
    "\n",
    "        ## define the LSTM\n",
    "        self.lstm = nn.LSTM(200, n_hidden, n_layers, \n",
    "                            dropout=drop_prob, batch_first=True)\n",
    "        \n",
    "        ## define a dropout layer\n",
    "        self.dropout = nn.Dropout(drop_prob)\n",
    "        \n",
    "        ## define the fully-connected layer\n",
    "        self.fc = nn.Linear(n_hidden, vocab_size)      \n",
    "    \n",
    "    def forward(self, x, hidden):\n",
    "        ''' Forward pass through the network. \n",
    "            These inputs are x, and the hidden/cell state `hidden`. '''\n",
    "\n",
    "        ## pass input through embedding layer\n",
    "        embedded = self.emb_layer(x)     \n",
    "        \n",
    "        ## Get the outputs and the new hidden state from the lstm\n",
    "        lstm_output, hidden = self.lstm(embedded, hidden)\n",
    "        \n",
    "        ## pass through a dropout layer\n",
    "        out = self.dropout(lstm_output)\n",
    "        \n",
    "        #out = out.contiguous().view(-1, self.n_hidden) \n",
    "        out = out.reshape(-1, self.n_hidden) \n",
    "\n",
    "        ## put \"out\" through the fully-connected layer\n",
    "        out = self.fc(out)\n",
    "\n",
    "        # return the final output and the hidden state\n",
    "        return out, hidden\n",
    "    \n",
    "    \n",
    "    def init_hidden(self, batch_size):\n",
    "        ''' initializes hidden state '''\n",
    "        # Create two new tensors with sizes n_layers x batch_size x n_hidden,\n",
    "        # initialized to zero, for hidden state and cell state of LSTM\n",
    "        weight = next(self.parameters()).data\n",
    "\n",
    "        # if GPU is available\n",
    "        if (torch.cuda.is_available()):\n",
    "            hidden = (weight.new(self.n_layers, batch_size, self.n_hidden).zero_().cuda(),\n",
    "                    weight.new(self.n_layers, batch_size, self.n_hidden).zero_().cuda())\n",
    "        \n",
    "        # if GPU is not available\n",
    "        else:\n",
    "            hidden = (weight.new(self.n_layers, batch_size, self.n_hidden).zero_(),\n",
    "                    weight.new(self.n_layers, batch_size, self.n_hidden).zero_())\n",
    "        \n",
    "        return hidden"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### instantiate the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WordLSTM(\n",
      "  (emb_layer): Embedding(371, 200)\n",
      "  (lstm): LSTM(200, 256, num_layers=4, batch_first=True, dropout=0.3)\n",
      "  (dropout): Dropout(p=0.3, inplace=False)\n",
      "  (fc): Linear(in_features=256, out_features=371, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "net = WordLSTM()\n",
    "\n",
    "# push the model to GPU (avoid it if you are not using the GPU)\n",
    "net.cuda()\n",
    "\n",
    "print(net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(net, epochs=10, batch_size=32, lr=0.001, clip=1, print_every=32):\n",
    "    \n",
    "    # optimizer\n",
    "    opt = torch.optim.Adam(net.parameters(), lr=lr)\n",
    "    \n",
    "    # loss\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    \n",
    "    # push model to GPU\n",
    "    net.cuda()\n",
    "    \n",
    "    counter = 0\n",
    "\n",
    "    net.train()\n",
    "\n",
    "    for e in range(epochs):\n",
    "\n",
    "        # initialize hidden state\n",
    "        h = net.init_hidden(batch_size)\n",
    "        \n",
    "        for x, y in get_batches(x_int, y_int, batch_size):\n",
    "            counter+= 1\n",
    "            \n",
    "            # convert numpy arrays to PyTorch arrays\n",
    "            inputs, targets = torch.from_numpy(x), torch.from_numpy(y)\n",
    "            \n",
    "            # push tensors to GPU\n",
    "            inputs, targets = inputs.cuda(), targets.cuda()\n",
    "\n",
    "            # detach hidden states\n",
    "            h = tuple([each.data for each in h])\n",
    "\n",
    "            # zero accumulated gradients\n",
    "            net.zero_grad()\n",
    "            \n",
    "            # get the output from the model\n",
    "            output, h = net(inputs, h)\n",
    "            \n",
    "            # calculate the loss and perform backprop\n",
    "            loss = criterion(output, targets.view(-1))\n",
    "\n",
    "            # back-propagate error\n",
    "            loss.backward()\n",
    "\n",
    "            # `clip_grad_norm` helps prevent the exploding gradient problem in RNNs / LSTMs.\n",
    "            nn.utils.clip_grad_norm_(net.parameters(), clip)\n",
    "\n",
    "            # update weigths\n",
    "            opt.step()            \n",
    "            \n",
    "            if counter % print_every == 0:\n",
    "            \n",
    "              print(\"Epoch: {}/{}...\".format(e+1, epochs),\n",
    "                    \"Step: {}...\".format(counter)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 3/20... Step: 50...\n",
      "Epoch: 5/20... Step: 100...\n",
      "Epoch: 8/20... Step: 150...\n",
      "Epoch: 10/20... Step: 200...\n",
      "Epoch: 13/20... Step: 250...\n",
      "Epoch: 15/20... Step: 300...\n",
      "Epoch: 18/20... Step: 350...\n",
      "Epoch: 20/20... Step: 400...\n"
     ]
    }
   ],
   "source": [
    "train(net, batch_size = 32, epochs=20, print_every=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### predict next token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(net, tkn, h=None):\n",
    "    \n",
    "         \n",
    "  # tensor inputs\n",
    "    x = np.array([[token2int[tkn]]])\n",
    "    inputs = torch.from_numpy(x)\n",
    "  \n",
    "  # push to GPU\n",
    "    inputs = inputs.cuda()\n",
    "\n",
    "  # detach hidden state from history\n",
    "    h = tuple([each.data for each in h])\n",
    "\n",
    "  # get the output of the model\n",
    "    out, h = net(inputs, h)\n",
    "\n",
    "  # get the token probabilities\n",
    "    p = F.softmax(out, dim=1).data\n",
    "\n",
    "    p = p.cpu()\n",
    "\n",
    "    p = p.numpy()\n",
    "    p = p.reshape(p.shape[1],)\n",
    "\n",
    "  # get indices of top 3 values\n",
    "    top_n_idx = p.argsort()[-3:][::-1]\n",
    "\n",
    "  # randomly select one of the three indices\n",
    "    sampled_token_index = top_n_idx[random.sample([0,1,2],1)[0]]\n",
    "\n",
    "  # return the encoded value of the predicted char and the hidden state\n",
    "    return int2token[sampled_token_index], h\n",
    "\n",
    "\n",
    "# function to generate text\n",
    "def sample(net, size, prime='it is'):\n",
    "    \n",
    "        \n",
    "    # push to GPU\n",
    "    net.cuda()\n",
    "    \n",
    "    net.eval()\n",
    "\n",
    "    # batch size is 1\n",
    "    h = net.init_hidden(1)\n",
    "\n",
    "    toks = prime.split()\n",
    "\n",
    "    # predict next token\n",
    "    for t in prime.split():\n",
    "        \n",
    "        token, h = predict(net, t, h)\n",
    "    \n",
    "    toks.append(token)\n",
    "\n",
    "    # predict subsequent tokens\n",
    "    for i in range(size-1):\n",
    "        token, h = predict(net, toks[-1], h)\n",
    "        toks.append(token)\n",
    "\n",
    "    return ' '.join(toks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'it is was heavy If choice pasta was be be portions and grana prices Target were There'"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample(net, 15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'one of the kitchen dining the food from other other had had had recent been Beverly residents prefer'"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample(net, 15, prime = \"one of the\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'as soon as by by in the almost warm but unobtrusive unobtrusive pm but described are described are'"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample(net, 15, prime = \"as soon as\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
